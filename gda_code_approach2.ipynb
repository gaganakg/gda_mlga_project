{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfUfSwFAi_KZ",
        "outputId": "31379659-c281-4a87-fc98-1e84ed7da7fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch has version 2.2.1+cu121\n",
            "12.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "print(\"PyTorch has version {}\".format(torch.__version__))\n",
        "print(torch.version.cuda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mb2keF8Yi55w",
        "outputId": "aa4ca039-d826-45ed-f5a5-502fe500148f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
            "Collecting torch-scatter\n",
            "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: torch-scatter\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-2.1.2-cp310-cp310-linux_x86_64.whl size=504020 sha256=a018c6613cacd7ff5d703143bd0bb23c6b1ef90b88f289af4f1945ee4b9eecd4\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/f1/2b/3b46d54b134259f58c8363568569053248040859b1a145b3ce\n",
            "Successfully built torch-scatter\n",
            "Installing collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
            "Collecting torch-sparse\n",
            "  Downloading torch_sparse-0.6.18.tar.gz (209 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.25.2)\n",
            "Building wheels for collected packages: torch-sparse\n"
          ]
        }
      ],
      "source": [
        "! pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "! pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "! pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XelsBPl7jJ7c"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import random\n",
        "import string\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "from plotly import graph_objs as go\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import metrics\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from torch_geometric.data import Data, download_url, extract_gz\n",
        "from torch_geometric.nn import GAE, GCNConv, VGAE\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "from torch_geometric.utils import train_test_split_edges, negative_sampling, degree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2UHQ28gj3i4"
      },
      "outputs": [],
      "source": [
        "# set the seeds\n",
        "torch.manual_seed(0)\n",
        "random.seed(0)\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muruwcb0j_po"
      },
      "outputs": [],
      "source": [
        "url = 'http://snap.stanford.edu/biodata/datasets/10012/files/DG-AssocMiner_miner-disease-gene.tsv.gz'\n",
        "extract_gz(download_url(url, '.'), '.')\n",
        "\n",
        "data_path = \"./DG-AssocMiner_miner-disease-gene.tsv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Krx6lh_vj9TO"
      },
      "outputs": [],
      "source": [
        "# Take a brief look at the structure of the data.\n",
        "# Note that column names are [\"# Disease ID\", \"Disease Name\", \"Gene ID\"].\n",
        "df = pd.read_csv(data_path, sep=\"\\t\")\n",
        "print(df.head(), '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LggDURMZkabY"
      },
      "outputs": [],
      "source": [
        "def load_node_mapping(datafile_path, index_col, offset=0):\n",
        "  df = pd.read_csv(datafile_path, index_col=index_col, sep=\"\\t\")\n",
        "  mapping = {index_id: i + offset for i, index_id in enumerate(df.index.unique())}\n",
        "  return mapping\n",
        "\n",
        "def load_edge_list(datafile_path, src_col, src_mapping, dst_col, dst_mapping):\n",
        "  df = pd.read_csv(datafile_path, sep=\"\\t\")\n",
        "  src_nodes = [src_mapping[index] for index in df[src_col]]\n",
        "  dst_nodes = [dst_mapping[index] for index in df[dst_col]]\n",
        "  edge_index = torch.tensor([src_nodes, dst_nodes])\n",
        "  return edge_index\n",
        "\n",
        "def initialize_data(datafile_path, num_features=1):\n",
        "  # Get disease node mapping and gene node mapping.\n",
        "  # Each node type has its own set of integer ids.\n",
        "  dz_col, gene_col = \"# Disease ID\", \"Gene ID\"\n",
        "  dz_mapping = load_node_mapping(datafile_path, dz_col, offset=0)\n",
        "  gene_mapping = load_node_mapping(datafile_path, gene_col, offset=519)\n",
        "\n",
        "  # Get edge index in terms of the integer indeces assigned to the nodes.\n",
        "  edge_index = load_edge_list(\n",
        "      datafile_path, dz_col, dz_mapping, gene_col, gene_mapping)\n",
        "\n",
        "  # Add the reverse direction (aka make it a undirected graph)\n",
        "  rev_edge_index = load_edge_list(\n",
        "      datafile_path, gene_col, gene_mapping, dz_col, dz_mapping)\n",
        "\n",
        "  # Construct a Data object.\n",
        "  data = Data()\n",
        "  data.num_nodes = len(dz_mapping) + len(gene_mapping)\n",
        "  data.edge_index = torch.cat((edge_index, rev_edge_index), dim=1)\n",
        "  # pretend we have uniform node features\n",
        "  data.x = torch.ones((data.num_nodes, num_features))\n",
        "\n",
        "  return data, gene_mapping, dz_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u58wJwuvkhUB"
      },
      "outputs": [],
      "source": [
        "# Read data and construct Data object.\n",
        "data_object, gene_mapping, dz_mapping = initialize_data(data_path)\n",
        "print(data_object)\n",
        "print(\"Number of genes:\", len(gene_mapping))\n",
        "print(\"Number of diseases:\", len(dz_mapping))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSQdeRlCmzje"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4Oe0QV0m1Sw"
      },
      "outputs": [],
      "source": [
        "transform = T.Compose([\n",
        "    T.NormalizeFeatures(),\n",
        "    T.ToDevice(device),\n",
        "    T.RandomLinkSplit(num_val=0.05, num_test=0.15, is_undirected=True,\n",
        "                      split_labels=True, add_negative_train_samples=True),\n",
        "])\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = transform(data_object)\n",
        "print(\"Train Data:\\n\", train_dataset)\n",
        "print(\"Validation Data:\\n\", val_dataset)\n",
        "print(\"Test Data:\\n\", test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptdMGTuQnBFU"
      },
      "outputs": [],
      "source": [
        "def get_mapping():\n",
        "  data_path = \"./DG-AssocMiner_miner-disease-gene.tsv\"\n",
        "  df = pd.read_csv(data_path, index_col=\"Disease Name\", sep=\"\\t\")\n",
        "  disease_mapping = [index_id for index_id in enumerate(df.index.unique())]\n",
        "  df = pd.read_csv(data_path, index_col=\"Gene ID\", sep=\"\\t\")\n",
        "  gene_mapping = [index_id[1] for index_id in enumerate(df.index.unique())]\n",
        "  mapping = disease_mapping + gene_mapping\n",
        "  return mapping\n",
        "\n",
        "def visualize_tsne_embeddings(model, data, title, perplexity=30.0,\n",
        "                              labeled=False, labels=[]):\n",
        "  \"\"\"Visualizes node embeddings in 2D space with t-SNE.\n",
        "\n",
        "  Args: model, pass in the trained or untrained model\n",
        "        data, Data object, where we assume the first 519 datapoints are disease\n",
        "          nodes and the rest are gene nodes\n",
        "        title, title of the plot\n",
        "        perplexity, t-SNE hyperparameter for perplexity\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  x = data.x\n",
        "  z = model.encode(x, data.edge_index)\n",
        "  ax1, ax2 = zip(*TSNE(n_components=2, learning_rate='auto', perplexity=perplexity,\n",
        "                       init='random').fit_transform(z.detach().cpu().numpy()))\n",
        "\n",
        "  fig = px.scatter(x=ax1, y=ax2, color=['r']*519 + ['g']*7294,\n",
        "                   hover_data=[get_mapping()],\n",
        "                   title=title)\n",
        "\n",
        "  if labeled:\n",
        "    for i in labels:\n",
        "      fig.add_annotation(x=ax1[i], y=ax2[i],\n",
        "                         text=str(i), showarrow=False)\n",
        "  fig.show()\n",
        "\n",
        "def visualize_pca_embeddings(model, data, title, labeled=False, labels=[]):\n",
        "  \"\"\"Visualizes node embeddings in 2D space with PCA (components=2)\n",
        "\n",
        "  Args: model, pass in the trained or untrained model\n",
        "        data, Data object, where we assume the first 519 datapoints are disease\n",
        "          nodes and the rest are gene nodes\n",
        "        title, title of the plot\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  x = data.x\n",
        "  z = model.encode(x, data.edge_index)\n",
        "\n",
        "  pca = PCA(n_components=2)\n",
        "  components = pca.fit_transform(z.detach().cpu().numpy())\n",
        "  fig = px.scatter(components, x=0, y=1, color=['r']*519 + ['g']*7294,\n",
        "                   hover_data=[get_mapping()], title=title)\n",
        "\n",
        "  if labeled:\n",
        "    for i in labels:\n",
        "      fig.add_annotation(x=components[:,0][i], y=components[:,1][i],\n",
        "                         text=str(i), showarrow=False)\n",
        "  fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pf9VKQ4onNLS"
      },
      "outputs": [],
      "source": [
        "def plot_roc_curve(model, data):\n",
        "  \"\"\"Visualizes ROC curve of model predictions\n",
        "\n",
        "  Args: model, pass in the trained or untrained model\n",
        "        data, Data object, where we assume the first 519 datapoints are disease\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "\n",
        "  x = data.x\n",
        "  z = model.encode(x, data.edge_index)\n",
        "\n",
        "  pos_preds = model.decode(z, data.pos_edge_label_index, sigmoid=True)\n",
        "  neg_preds = model.decode(z, data.neg_edge_label_index, sigmoid=True)\n",
        "  preds = torch.cat([pos_preds, neg_preds], dim=0)\n",
        "  preds = preds.detach().cpu().numpy()\n",
        "\n",
        "  labels = torch.cat((data.pos_edge_label, data.neg_edge_label), dim=0)\n",
        "  labels = labels.detach().cpu().numpy()\n",
        "  fpr, tpr, thresholds = metrics.roc_curve(labels, preds)\n",
        "\n",
        "  # Using J-statistic: https://en.wikipedia.org/wiki/Youden%27s_J_statistic\n",
        "  J = tpr - fpr\n",
        "  ix = np.argmax(J)\n",
        "  best_thresh = thresholds[ix]\n",
        "  print('Best Threshold=%f' % (best_thresh))\n",
        "\n",
        "  roc_auc = metrics.roc_auc_score(labels, preds)\n",
        "\n",
        "  plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "  plt.plot([0, 1], [0, 1],'r--') # diagonal roc curve of a random classifier\n",
        "  plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best=%0.2f' % best_thresh)\n",
        "  plt.xlim(0, 1)\n",
        "  plt.ylim(0, 1)\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.legend(loc = 'lower right')\n",
        "  plt.title('ROC curve for model predictions')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5n7mKG-nQIP"
      },
      "outputs": [],
      "source": [
        "def plot_training_stats(title, losses, test_auc, test_ap, train_auc, train_ap):\n",
        "  \"\"\"Plots evolution of loss and metrics during training\n",
        "\n",
        "  Args: losses, test_auc, test_ap, train_auc, and train_ap should be lists\n",
        "    outputted by the training process.\n",
        "  \"\"\"\n",
        "  fig, ax = plt.subplots()\n",
        "  ax2 = ax.twinx()\n",
        "\n",
        "  ax.set_xlabel(\"Training Epochs\")\n",
        "  ax2.set_ylabel(\"Performance Metric\")\n",
        "  ax.set_ylabel(\"Loss\")\n",
        "\n",
        "  plt.title(title)\n",
        "  p1, = ax.plot(losses, \"b-\", label=\"training loss\")\n",
        "  p2, = ax2.plot(test_auc, \"r-\", label=\"test AUC\")\n",
        "  p3, = ax2.plot(test_ap, \"g-\", label=\"test AP\")\n",
        "  p4, = ax2.plot(train_auc, \"o-\", label=\"train AUC\")\n",
        "  p5, = ax2.plot(train_ap, \"v-\", label=\"train AP\")\n",
        "  plt.legend(handles=[p1, p2, p3, p4, p5])\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwCVdWpqlJ2V"
      },
      "outputs": [],
      "source": [
        "def get_edge_dot_products(data, model, num_dz_nodes=519):\n",
        "  \"\"\"\n",
        "  A pair of nodes (u,v) is predicted to be connected with an edge if the dot\n",
        "  product between the learned embeddings of u and v is high. This function\n",
        "  computes and returns the dot product of all pairs of (dz_node, gene_node).\n",
        "\n",
        "  Args:\n",
        "    data, the data_object containing the original node featues\n",
        "    model, the model that will be used to encode the data\n",
        "    num_dz_nodes, the number of disease nodes; used to differentiate between\n",
        "      disease and gene node embeddings\n",
        "  Returns:\n",
        "    dot_products, a numpy 2D array of shape (num_dz_nodes, num_gene_nodes)\n",
        "      containing the dot product between each (dz_node, gene_node) pair.\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  x = data.x\n",
        "  z = model.encode(x, data.edge_index).detach().numpy()\n",
        "  dz_z = z[:num_dz_nodes, :]\n",
        "  gene_z = z[num_dz_nodes:, :]\n",
        "\n",
        "  dot_products = np.einsum('ai,bi->ab', dz_z, gene_z)\n",
        "  return dot_products   # numpy array of shape (num_dz_nodes, num_gene_nodes)\n",
        "\n",
        "\n",
        "def get_ranked_edges(data_object, model, num_dz_nodes=519):\n",
        "  \"\"\"\n",
        "  Ranks all potential edges as predicted by the model.\n",
        "\n",
        "  Args:\n",
        "    data, the data_object containing the original node featues\n",
        "    model, the model that will be used to encode the data\n",
        "    num_dz_nodes, the number of disease nodes; used to differentiate between\n",
        "      disease and gene node embeddings\n",
        "  Returns:\n",
        "    ranked_edge_list, a full edge list ranked by the likelihood of the edge\n",
        "      being a positive edge, in decreasing order\n",
        "    ranked_dot_products, a list of the dot products of each edge's node\n",
        "      embeddings, ranked in decreasing order\n",
        "  \"\"\"\n",
        "  # Get dot products\n",
        "  edge_dot_products = get_edge_dot_products(data_object, model, num_dz_nodes=num_dz_nodes)\n",
        "  num_possible_edges = edge_dot_products.shape[0] * edge_dot_products.shape[1]\n",
        "\n",
        "  # Get indeces, ranked by dot product in descending order. This is a tuple (indeces[0], indeces[1]).\n",
        "  ranked_edges = np.unravel_index(np.argsort(-1 * edge_dot_products, axis=None), edge_dot_products.shape)\n",
        "  assert len(ranked_edges[0]) == num_possible_edges\n",
        "\n",
        "  # Get the corresponding, ranked edge list and ranked dot products. Note that\n",
        "  # we need to add an offset for the gene_node indeces.\n",
        "  offset = np.array([np.zeros(num_possible_edges, dtype=int), num_dz_nodes + np.ones(num_possible_edges, dtype=int)]).T\n",
        "  ranked_edge_list = np.dstack(ranked_edges)[0] + offset\n",
        "  assert ranked_edge_list.shape[0] == num_possible_edges\n",
        "\n",
        "  # Get the corresponding ranked dot products\n",
        "  ranked_dot_products = edge_dot_products[ranked_edges]\n",
        "  assert ranked_dot_products.shape[0] == num_possible_edges\n",
        "\n",
        "  return ranked_edge_list, ranked_dot_products\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-pMD99MAEQO"
      },
      "outputs": [],
      "source": [
        "#@title Dataset Configuration\n",
        "\n",
        "NUM_FEATURES =   20#@param {type: \"integer\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0-WBd2TAG8b"
      },
      "outputs": [],
      "source": [
        "data_object.x = torch.ones((data_object.num_nodes, NUM_FEATURES))\n",
        "print(\"Using dummy embeddings as initial node features.\")\n",
        "print(\"Number of features set to \", NUM_FEATURES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGAX5GslozPL"
      },
      "outputs": [],
      "source": [
        "#@title Model & Training Configuration\n",
        "\n",
        "HIDDEN_SIZE = 200  #@param {type: \"integer\"}\n",
        "\n",
        "OUT_CHANNELS = 20  #@param {type: \"integer\"}\n",
        "\n",
        "EPOCHS =   40#@param {type: \"integer\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoKvDAUoo5k2"
      },
      "outputs": [],
      "source": [
        "class VariationalGCNEncoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_size, out_channels, dropout):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_size, cached=False)\n",
        "        self.conv_mu = GCNConv(hidden_size, out_channels, cached=False)\n",
        "        self.conv_logstd = GCNConv(hidden_size, out_channels)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x_temp1 = self.conv1(x, edge_index).relu()\n",
        "        x_temp2 = self.dropout(x_temp1)\n",
        "        return self.conv_mu(x_temp2, edge_index), self.conv_logstd(x_temp2, edge_index)\n",
        "\n",
        "\n",
        "vgae_model = VGAE(VariationalGCNEncoder(NUM_FEATURES, HIDDEN_SIZE, OUT_CHANNELS, dropout=0.5))\n",
        "vgae_model = vgae_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arEiiqzFo_Ty"
      },
      "outputs": [],
      "source": [
        "def vgae_train(train_data, vgae_model, optimizer):\n",
        "    optimizer.zero_grad()\n",
        "    vgae_model.train()\n",
        "    z = vgae_model.encode(train_data.x, train_data.edge_index)\n",
        "    loss = (\n",
        "        vgae_model.recon_loss(z, train_data.pos_edge_label_index.to(device)) +\n",
        "        (1 / train_data.num_nodes) * vgae_model.kl_loss()\n",
        "    )\n",
        "    loss.backward(retain_graph=True)\n",
        "    optimizer.step()\n",
        "    return float(loss)\n",
        "\n",
        "@torch.no_grad()\n",
        "def vgae_test(test_data, vgae_model):\n",
        "    vgae_model.eval()\n",
        "    z = vgae_model.encode(test_data.x, test_data.edge_index)\n",
        "    return vgae_model.test(z, test_data.pos_edge_label_index,\n",
        "                      test_data.neg_edge_label_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJcdsLggpVKS"
      },
      "outputs": [],
      "source": [
        "gene_ids_data_path = \"./gene_ids.tsv\"\n",
        "\n",
        "gene_ids_df = pd.read_csv(gene_ids_data_path, sep=\"\\t\")\n",
        "gene_ids_df = gene_ids_df.rename(columns={\n",
        "    \"Gene stable ID\": \"ENSEMBL Gene ID\",\n",
        "    \"Gene stable ID version\": \"ENSEMBL Gene ID Version\",\n",
        "    \"NCBI gene (formerly Entrezgene) ID\": \"Gene ID\"})\n",
        "gene_ids_df = gene_ids_df.loc[:, [\"ENSEMBL Gene ID\", \"Gene ID\"]]\n",
        "\n",
        "# Add new \"ENSEMBL Gene ID\" column to our existing Gene-Disease Assoc table.\n",
        "new_df = pd.merge(df, gene_ids_df, left_on=\"Gene ID\", right_on=\"Gene ID\", how=\"left\")\n",
        "new_df[\"Gene ID\"] = new_df[\"Gene ID\"].astype(int)\n",
        "print(new_df.loc[:, [\"Gene ID\", \"ENSEMBL Gene ID\", ]])  # To confirm mapping.\n",
        "\n",
        "\n",
        "# Check how many NCBI Gene IDs were unable to be mapped to ENSEMBL Genes IDs.\n",
        "print(\"Number of unmapped NCBI Genes:\",\n",
        "      new_df.drop_duplicates(subset=\"Gene ID\")['ENSEMBL Gene ID'].isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99fgYkiNp-kj"
      },
      "outputs": [],
      "source": [
        "# Download the GeneSynopysis dataset directly from the BioSNAP site.\n",
        "url = 'https://snap.stanford.edu/biodata/datasets/10022/files/G-SynMiner_miner-geneHUGO.tsv.gz'\n",
        "extract_gz(download_url(url, '.'), '.')\n",
        "\n",
        "# This table contains info on a select set of genes, with each gene identified\n",
        "# via their ENSEMBL Gene Id or HGNC Id. We would like to use the \"name\" as the\n",
        "# short description of the gene.\n",
        "data_path = \"./G-SynMiner_miner-geneHUGO.tsv.gz\"\n",
        "genes_df = pd.read_csv(data_path, sep=\"\\t\")\n",
        "# print('\\n', genes_df.loc[:,[\"# ensembl_gene_id\", \"name\"]])\n",
        "\n",
        "# Add the appropriate \"name\" data to the dataframe for each gene.\n",
        "full_df = pd.merge(\n",
        "    new_df, genes_df, left_on=\"ENSEMBL Gene ID\",\n",
        "    right_on=\"# ensembl_gene_id\", how=\"left\")\n",
        "\n",
        "# Cut out any extraneous columns from the data frame and rename for easier use.\n",
        "full_df = full_df.loc[:, ['# Disease ID', 'Disease Name', 'Gene ID', 'name']]\n",
        "full_df = full_df.rename(columns={\n",
        "    '# Disease ID': \"Disease ID\",\n",
        "    'Disease Name': \"Disease Name\",\n",
        "    'Gene ID': \"Gene ID\",\n",
        "    'name': \"Gene Name\",\n",
        "})\n",
        "\n",
        "# print(full_df)\n",
        "# print(full_df.columns)\n",
        "print(\"Gene ID <-> Gene Name:\\n\", full_df.loc[:, [\"Gene ID\", \"Gene Name\"]], '\\n')\n",
        "\n",
        "num_nan_ensembl = new_df.drop_duplicates(subset=\"Gene ID\")['ENSEMBL Gene ID'].isna().sum()\n",
        "num_nan_desc = full_df.drop_duplicates(subset=\"Gene ID\")[\"Gene Name\"].isna().sum()\n",
        "print(\"Number of missing ENSEMBL Gene IDs:\", num_nan_ensembl)\n",
        "print(\"Number of missing gene names:\", num_nan_desc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6Vla33DqFt5"
      },
      "outputs": [],
      "source": [
        "def generate_encodings(data_frame, id_col, description_col, node_mapping):\n",
        "  # Prepare the description data frame.\n",
        "  desc_df = data_frame.loc[:, [id_col, description_col]]\n",
        "  desc_df = desc_df.drop_duplicates(subset=id_col)\n",
        "  desc_df = desc_df[desc_df[description_col].notnull()]\n",
        "\n",
        "  # Initialize word encoding, and the node encodings.\n",
        "  num_encoded_words, num_nan, num_nodes = 0, 0, len(node_mapping)\n",
        "  word_encoding, node_encodings = {}, {}\n",
        "  excluded_words = [str(i) for i in range(10)]\n",
        "\n",
        "  # Iterate over the nodes accounted for in node_mapping.\n",
        "  for node_id, node_index in node_mapping.items():\n",
        "    # Only attempt to encode if a description is available.\n",
        "    if node_id not in desc_df[id_col].values:\n",
        "      num_nan += 1\n",
        "      continue\n",
        "\n",
        "    # Get description and break it down to its words.\n",
        "    description = desc_df.loc[desc_df[id_col] == node_id][description_col].values[0]\n",
        "    words = [\n",
        "      word.strip().lower().translate(str.maketrans('', '', string.punctuation))\n",
        "      for word in description.split()\n",
        "    ]\n",
        "    filtered_words = [\n",
        "      word for word in words if word not in excluded_words\n",
        "    ]\n",
        "\n",
        "    # Encode the words.\n",
        "    encoding = []\n",
        "    for word in filtered_words:\n",
        "      if word not in word_encoding:\n",
        "        word_encoding[word] = num_encoded_words\n",
        "        num_encoded_words += 1\n",
        "      encoding.append(word_encoding[word])\n",
        "    node_encodings[node_index] = encoding\n",
        "\n",
        "  # Order the encodings as the nodes are ordered in node_mapping.\n",
        "  final_encoding = [\n",
        "      node_encodings[i] if i in node_encodings else []\n",
        "      for i in range(num_nodes)\n",
        "  ]\n",
        "  print(\"Number of nodes without a description:\", num_nan)\n",
        "  return final_encoding, word_encoding\n",
        "\n",
        "\n",
        "# Encode genes by the one-hot encodings of their descriptions.\n",
        "# The |gene_encodings| should be in the same order as the genes in |gene_mapping|\n",
        "print(\"== GENE NODE ENCODING ==\")\n",
        "gene_encodings, gene_word_encoding = generate_encodings(\n",
        "    full_df, \"Gene ID\", \"Gene Name\", gene_mapping)\n",
        "print(\"Number of nodes processed:\", len(gene_encodings))\n",
        "print(\"Number of words:\", len(gene_word_encoding))\n",
        "\n",
        "# Encode diseases by the one-hot encodings of their descriptions.\n",
        "# The |dz_encodings| should be in the same order as the genes in |dz_mapping|\n",
        "print(\"\\n== DISEASE NODE ENCODING ==\")\n",
        "dz_encodings, dz_word_encoding = generate_encodings(\n",
        "    full_df, \"Disease ID\", \"Disease Name\", dz_mapping\n",
        ")\n",
        "print(\"Number of nodes processed:\", len(dz_encodings))\n",
        "print(\"Number of node words:\", len(dz_word_encoding))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwB0uuSh8c1m"
      },
      "outputs": [],
      "source": [
        "#@title Description Embedding Configuration\n",
        "\n",
        "GENE_EMB_SIZE =   25#@param {type: \"integer\"}\n",
        "\n",
        "DISEASE_EMB_SIZE = 15#@param {type: \"integer\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DP8GsGDsqQR7"
      },
      "outputs": [],
      "source": [
        "class WordToVecModeler(nn.Module):\n",
        "  # This basic module is roughly based off the tutorial at\n",
        "  # https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(WordToVecModeler, self).__init__()\n",
        "    self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.linear1 = nn.Linear(embedding_dim, 128)\n",
        "    self.linear2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "  def forward(self, encoded_phrase):\n",
        "    # |encoded_phrase|: encoded words of the input phrase\n",
        "    word_embeds = self.embeddings(encoded_phrase)  # List of each word's embedding.\n",
        "    phrase_embed = torch.sum(word_embeds, 0)\n",
        "    out = F.relu(self.linear1(phrase_embed))\n",
        "    out = self.linear2(out)\n",
        "    log_probs = F.log_softmax(out, dim=0)\n",
        "    return log_probs\n",
        "\n",
        "\n",
        "def generate_embeddings(node_encodings, node_word_encoding, embedding_dim):\n",
        "  num_words = len(node_word_encoding)\n",
        "\n",
        "  loss_function = nn.NLLLoss()\n",
        "  model = WordToVecModeler(num_words, embedding_dim)\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "  losses = []\n",
        "  for epoch in range(15):\n",
        "    total_loss = 0\n",
        "    for encoded_words in node_encodings:\n",
        "      if len(encoded_words) > 0:\n",
        "        model.zero_grad()\n",
        "        log_probs = model(torch.tensor(encoded_words, dtype=torch.long))\n",
        "\n",
        "        one_hot_vector = torch.zeros(num_words, dtype=torch.long)\n",
        "        for code in encoded_words:\n",
        "          one_hot_vector[code] = 1.0\n",
        "\n",
        "        loss = loss_function(log_probs, one_hot_vector)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
        "        total_loss += loss.item()\n",
        "    # print(epoch, total_loss)\n",
        "    losses.append(total_loss)\n",
        "  print(\"Losses (total at end of each iteration):\\n\", losses)  # The loss decreased every iteration over the training data!\n",
        "\n",
        "  return model.embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSUt_xx5qWP2"
      },
      "outputs": [],
      "source": [
        "print(\"Gene Emb Size:\", GENE_EMB_SIZE)\n",
        "gene_embedding = generate_embeddings(gene_encodings, gene_word_encoding, GENE_EMB_SIZE)\n",
        "\n",
        "print(\"\\nDisease Emb Size:\", DISEASE_EMB_SIZE)\n",
        "dz_embedding = generate_embeddings(dz_encodings, dz_word_encoding, DISEASE_EMB_SIZE)\n",
        "\n",
        "torch.save(gene_embedding, f=\"gene_embeds_\" + str(GENE_EMB_SIZE))\n",
        "torch.save(dz_embedding, f=\"dz_embeds_\" + str(DISEASE_EMB_SIZE))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2xUEzOhqXtF"
      },
      "outputs": [],
      "source": [
        "def compute_node_embeddings(embed_model, encodings):\n",
        "  # Get statistics of the embeddings.\n",
        "  weights = embed_model.weight.data.numpy()\n",
        "  means, stds = np.mean(weights, axis=0), np.std(weights, axis=0)\n",
        "\n",
        "  node_embeds = []\n",
        "  for encod in encodings:\n",
        "    # If encoding is empty, sample a random embedding.\n",
        "    if len(encod) == 0:\n",
        "      embed = torch.LongTensor(np.random.normal(loc=means, scale=stds))\n",
        "    else:\n",
        "      embed = torch.sum(embed_model(torch.LongTensor(encod)), 0)\n",
        "    node_embeds.append(embed)\n",
        "\n",
        "  # Return embeddings of each node.\n",
        "  embeddings = torch.vstack(node_embeds)\n",
        "  return embeddings, means, stds\n",
        "\n",
        "\n",
        "def initialize_featured_dataset(\n",
        "    gene_encodings, gene_embed_model, dz_encodings, dz_embed_model, edge_index):\n",
        "  # Get the embeddings for the gene and disease nodes.\n",
        "  gene_node_embeds, gene_means, gene_stds = compute_node_embeddings(gene_embed_model, gene_encodings)\n",
        "  dz_node_embeds, dz_means, dz_stds = compute_node_embeddings(dz_embed_model, dz_encodings)\n",
        "\n",
        "  num_gene_nodes, gene_embed_dim = gene_node_embeds.shape\n",
        "  num_dz_nodes, dz_embed_dim = dz_node_embeds.shape\n",
        "\n",
        "  # The node embedding space would be the gene embedding and disease embedding\n",
        "  # spaces combined. For the gene nodes, the disease embedding dimensions would\n",
        "  # be randoly sampled, and vice cersa for the disease nodes.\n",
        "  gene_node_dz_embeds = torch.LongTensor(\n",
        "      np.random.normal(loc=dz_means, scale=dz_stds, size=(num_gene_nodes, dz_embed_dim)))\n",
        "  gene_node_embeds = torch.hstack((gene_node_embeds, gene_node_dz_embeds))\n",
        "\n",
        "  dz_node_gene_embeds = torch.LongTensor(\n",
        "      np.random.normal(loc=gene_means, scale=gene_stds, size=(num_dz_nodes, gene_embed_dim)))\n",
        "  dz_node_embeds = torch.hstack((dz_node_gene_embeds, dz_node_embeds))\n",
        "\n",
        "  # Stack the embeddings to get the full node embeddings matrix, which would be\n",
        "  # the node features used for our GAE/VGAE models.\n",
        "  assert gene_node_embeds.shape[1] == dz_node_embeds.shape[1]\n",
        "  node_embeds = torch.vstack((gene_node_embeds, dz_node_embeds))\n",
        "\n",
        "  # Construct the homogenous data object.\n",
        "  data_object = Data(x=node_embeds, edge_index=edge_index)\n",
        "  return data_object\n",
        "\n",
        "\n",
        "gene_embed = torch.load(\"gene_embeds_\" + str(GENE_EMB_SIZE))\n",
        "dz_embed = torch.load(\"dz_embeds_\" + str(DISEASE_EMB_SIZE))\n",
        "\n",
        "print(\"Number of gene encodings:\", len(gene_encodings))\n",
        "print(\"Shape of gene node embeddings:\", gene_embed.weight.data.shape)\n",
        "\n",
        "print(\"Number of dz encodings:\", len(dz_encodings))\n",
        "print(\"Shape of disease node embeddings:\", dz_embed.weight.data.shape)\n",
        "\n",
        "data_object_with_features = initialize_featured_dataset(\n",
        "    gene_encodings, gene_embed, dz_encodings, dz_embed, data_object.edge_index\n",
        ")\n",
        "print(data_object_with_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjFJNcZ7CYvu"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-dF9uj1CZjo"
      },
      "outputs": [],
      "source": [
        "transform = T.Compose([\n",
        "    T.NormalizeFeatures(),\n",
        "    T.ToDevice(device),\n",
        "    T.RandomLinkSplit(num_val=0.05, num_test=0.15, is_undirected=True,\n",
        "                      split_labels=True, add_negative_train_samples=True),\n",
        "])\n",
        "\n",
        "train_dataset_with_features, val_dataset_with_features, test_dataset_with_features = transform(data_object_with_features)\n",
        "print(\"Train Data w/ Description Embeddings:\\n\", train_dataset_with_features)\n",
        "print(\"Validation Data w/ Description Embeddings:\\n\", val_dataset_with_features)\n",
        "print(\"Test Data w/ Description Embeddings:\\n\", test_dataset_with_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLDs6Bzr5__r"
      },
      "outputs": [],
      "source": [
        "#@title Model & Training Configuration\n",
        "\n",
        "MODEL_NAME = \"VGAE\" #@param [\"GAE\", \"VGAE\"] {type:\"string\"}\n",
        "\n",
        "HIDDEN_SIZE = 200  #@param {type: \"integer\"}\n",
        "\n",
        "OUT_CHANNELS = 20  #@param {type: \"integer\"}\n",
        "\n",
        "EPOCHS =   60#@param {type: \"integer\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nd8bkiZyqep2"
      },
      "outputs": [],
      "source": [
        "if MODEL_NAME == \"GAE\":\n",
        "  selected_model = GAE(GCNEncoder(GENE_EMB_SIZE + DISEASE_EMB_SIZE, HIDDEN_SIZE, OUT_CHANNELS, 0.5))\n",
        "elif MODEL_NAME == \"VGAE\":\n",
        "  selected_model = VGAE(VariationalGCNEncoder(GENE_EMB_SIZE + DISEASE_EMB_SIZE, HIDDEN_SIZE, OUT_CHANNELS, 0.5))\n",
        "\n",
        "print(\"Model selected:\")\n",
        "selected_model = selected_model.to(device)\n",
        "print(selected_model)\n",
        "\n",
        "optimizer = torch.optim.Adam(selected_model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKFkS1BaBuDh"
      },
      "outputs": [],
      "source": [
        "losses = []\n",
        "test_auc = []\n",
        "test_ap = []\n",
        "train_aucs = []\n",
        "train_aps = []\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  if MODEL_NAME == \"GAE\":\n",
        "    loss = gae_train(train_dataset_with_features, selected_model, optimizer)\n",
        "    auc, ap = gae_test(test_dataset_with_features, selected_model)\n",
        "    train_auc, train_ap = gae_test(train_dataset_with_features, selected_model)\n",
        "  else:\n",
        "    loss = vgae_train(train_dataset_with_features, selected_model, optimizer)\n",
        "    auc, ap = vgae_test(test_dataset_with_features, selected_model)\n",
        "    train_auc, train_ap = vgae_test(train_dataset_with_features, selected_model)\n",
        "\n",
        "  losses.append(loss)\n",
        "  test_auc.append(auc)\n",
        "  test_ap.append(ap)\n",
        "  train_aucs.append(train_auc)\n",
        "  train_aps.append(train_ap)\n",
        "\n",
        "  print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}, tAUC: {:.4f}, tAP: {:.4f}, loss:{:.4f}'.format(epoch, auc, ap, train_auc, train_ap, loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvhEU8CNiWAL"
      },
      "outputs": [],
      "source": [
        "plot_training_stats(MODEL_NAME + \" with description embeddings\", losses, test_auc, test_ap, train_aucs, train_aps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5B3xMjFY1fRy"
      },
      "outputs": [],
      "source": [
        "plot_roc_curve(selected_model, test_dataset_with_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zP3_AZzo1ssD"
      },
      "outputs": [],
      "source": [
        "visualize_tsne_embeddings(\n",
        "    selected_model, train_dataset_with_features,\n",
        "    title='Trained Model (using description embeddings): train set embeddings',\n",
        "    labeled=True, labels=[40, 190, 230, 1830, 260, 110, 280, 1967])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pl_X9TIJ2QS8"
      },
      "outputs": [],
      "source": [
        "visualize_pca_embeddings(\n",
        "    selected_model, train_dataset_with_features,\n",
        "    title='Trained Model (using description embeddings): train set embeddings',\n",
        "    labeled=True, labels=[40, 190, 230, 1830, 260, 110, 280, 1967])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VL3fQw61Lg33"
      },
      "outputs": [],
      "source": [
        "# Select particular model and dataset\n",
        "data_object_to_analyze = data_object    # Choose from: data_object, data_object_with_features\n",
        "model_to_analyze = vgae_model            # Choose from: gae_model, vgae_model, selected_model\n",
        "\n",
        "print(\"DataObject:\\n\", data_object_to_analyze)\n",
        "print(\"\\nModel:\\n\", model_to_analyze)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQvCtJF2eW59"
      },
      "outputs": [],
      "source": [
        "# Get ranked edge list, filtered by the selected node ids.\n",
        "ranked_edge_list, ranked_dot_products = get_ranked_edges(data_object_to_analyze, model_to_analyze)\n",
        "\n",
        "# Plot histogram of the dot products\n",
        "plt.hist(ranked_dot_products, bins=20)\n",
        "plt.title(\"Histogram of Dot Products for each Pair of Nodes\")\n",
        "plt.yscale('log')\n",
        "plt.ylabel(\"Frequency (log scale)\")\n",
        "plt.xlabel(\"Dot Product of pairs of nodes\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWbvmDjydCDm"
      },
      "outputs": [],
      "source": [
        "# Select for particular examples\n",
        "select_gene_substrings = [\"BRCA\"]\n",
        "select_disease_substrings = [\"Alzheimer's Disease\",\"Cancer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfISS5iVN0Dr"
      },
      "outputs": [],
      "source": [
        "# Filter data frame by the selected disease terms, and get their node ids.\n",
        "if select_disease_substrings:\n",
        "  dz_regex = \"(?i)\" + \"|\".join(select_disease_substrings)\n",
        "  query_df = full_df[full_df['Disease Name'].str.contains(dz_regex)]\n",
        "  query_dz_nodes = [dz_mapping[dz_id] for dz_id in query_df['Disease ID'].drop_duplicates().tolist()]\n",
        "  # print(\"\\nQueried Disease Nodes:\\n\", query_df['Disease Name'])\n",
        "else:\n",
        "  query_dz_nodes = None\n",
        "\n",
        "# Filter data frame by the selected gene terms, and get their node ids.\n",
        "if select_gene_substrings:\n",
        "  gene_regex = \"(?i)\" + \"|\".join(select_gene_substrings)\n",
        "  query_df = full_df[full_df['Gene Name'].notna()]\n",
        "  query_df = query_df[query_df['Gene Name'].str.contains(gene_regex)]\n",
        "  query_gene_nodes = [gene_mapping[gene_id] for gene_id in query_df['Gene ID'].drop_duplicates().tolist()]\n",
        "  # print(\"\\nQueried Gene Nodes:\\n\", query_df['Gene Name'])\n",
        "else:\n",
        "  query_gene_nodes = None\n",
        "\n",
        "\n",
        "# Get reverse dz and gene mappings, to print out all needed info.\n",
        "reverse_dz_mapping = {j: i for i,j in dz_mapping.items()}\n",
        "reverse_gene_mapping = {j: i for i,j in gene_mapping.items()}\n",
        "\n",
        "print(\"\\nTop Predicted Edges\")\n",
        "top_k = 50\n",
        "curr_k = 0\n",
        "for dz_i, gene_i in ranked_edge_list:\n",
        "  # Skip edges that do not include the selected dz and gene nodes\n",
        "  if query_dz_nodes and dz_i not in query_dz_nodes:\n",
        "    continue\n",
        "  if query_gene_nodes and gene_i not in query_gene_nodes:\n",
        "    continue\n",
        "\n",
        "  # Get all the info needed (dz_i and gene_i are the node indeces)\n",
        "  dz_id, gene_id = reverse_dz_mapping[dz_i], reverse_gene_mapping[gene_i]\n",
        "  dz_description = full_df[full_df[\"Disease ID\"] == dz_id][\"Disease Name\"].drop_duplicates().iloc[0]\n",
        "  gene_description = full_df[full_df[\"Gene ID\"] == gene_id][\"Gene Name\"].drop_duplicates().iloc[0]\n",
        "  dot_product = ranked_dot_products[curr_k]\n",
        "\n",
        "  print('edge=({},{}), \\t dotprod={:.2f},\\t descriptions=({},{})'.format(dz_i, gene_i, dot_product, dz_description, gene_description))\n",
        "\n",
        "  curr_k += 1\n",
        "  if curr_k > top_k:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPoqRHyORg0X"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create an empty graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes for diseases and genes\n",
        "if query_dz_nodes:\n",
        "    for dz_node in query_dz_nodes:\n",
        "        for disease_substring in select_disease_substrings:\n",
        "            if disease_substring.lower() in full_df[full_df[\"Disease ID\"] == reverse_dz_mapping[dz_node]][\"Disease Name\"].iloc[0].lower():\n",
        "                G.add_node(dz_node, type='Disease', label=disease_substring)\n",
        "                break\n",
        "if query_gene_nodes:\n",
        "    G.add_nodes_from(query_gene_nodes, type='Gene')    # Genes are represented as nodes with type 'Gene'\n",
        "\n",
        "# Add edges\n",
        "for dz_i, gene_i in ranked_edge_list:\n",
        "    if query_dz_nodes and dz_i not in query_dz_nodes:\n",
        "        continue\n",
        "    if query_gene_nodes and gene_i not in query_gene_nodes:\n",
        "        continue\n",
        "    G.add_edge(dz_i, gene_i)  # Edges connect diseases (represented by dz_i) with genes (represented by gene_i)\n",
        "\n",
        "# Draw the graph with selected nodes\n",
        "plt.figure(figsize=(12, 8))\n",
        "pos = nx.spring_layout(G)  # Position nodes using Fruchterman-Reingold force-directed algorithm\n",
        "\n",
        "# Draw disease nodes\n",
        "for disease_substring in select_disease_substrings:\n",
        "    disease_nodes = [node for node, attr in G.nodes(data=True) if attr['type'] == 'Disease' and attr['label'] == disease_substring]\n",
        "    if disease_substring.lower() == \"alzheimer's disease\":\n",
        "        node_color = 'lightblue'\n",
        "    elif disease_substring.lower() == \"cancer\":\n",
        "        node_color = 'lightcoral'\n",
        "    nx.draw_networkx_nodes(G, pos, nodelist=disease_nodes, node_color=node_color, node_size=500, label=disease_substring)\n",
        "\n",
        "# Draw gene nodes\n",
        "gene_nodes = [node for node, attr in G.nodes(data=True) if attr['type'] == 'Gene']\n",
        "nx.draw_networkx_nodes(G, pos, nodelist=gene_nodes, node_color='lightgreen', node_size=500, label='Gene')\n",
        "\n",
        "# Draw edges\n",
        "nx.draw_networkx_edges(G, pos)\n",
        "\n",
        "# Draw labels\n",
        "nx.draw_networkx_labels(G, pos, font_size=10)\n",
        "\n",
        "# Set title and legend\n",
        "plt.title('Predicted Disease-Gene Network (Alzheimer\\'s Disease, Cancer, BRCA)')\n",
        "plt.legend()\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThKjV5THU9Tr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqBo8kYYU9Wb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNx7G2aiU9Y0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7x90ayulU9bS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mh16s7FU9do"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ND7_uRMU9f4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpYbUa6BU9ik"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}